## Scalable ML Data Pipeline using Hadoop and Spark

**TL;DR**: An end-to-end distributed data pipeline for large-scale e-commerce user behavior analysis, built using Hadoop, Spark, and Docker to support scalable analytics and downstream machine learning workflows.

This project demonstrates how raw user interaction data can be ingested, stored, processed, and analyzed using big data technologies in a reproducible, production-style setup.

This is an academic project completed during **Spring 2026** as part of the **Data-Intensive Computing** course in the Master’s in Artificial Intelligence program at the University at Buffalo.

---

### Project Overview

Modern ML systems rely on robust data pipelines for scalable data ingestion and preprocessing.  
This project focuses on building an end-to-end pipeline that enables:

- Distributed storage of large datasets
- Scalable batch processing using Spark
- Structured data preparation for analytics and ML use cases

---

### Pipeline Architecture

- **Data Source**: E-commerce user behavior dataset  
- **Storage**: Hadoop Distributed File System (HDFS)  
- **Processing**: Apache Spark (PySpark)  
- **Orchestration**: Docker & Docker Compose  
- **Execution Environment**: Containerized multi-service setup  

---

### Key Components

- Automated data ingestion into HDFS
- Spark-based transformations and aggregations
- Validation of distributed file storage
- Reproducible environment using Docker

---

### Technology Stack

- Python  
- Apache Hadoop (HDFS)  
- Apache Spark (PySpark)  
- Docker & Docker Compose  
- Shell scripting  

---

### Authors

**Kanisha Raja**  
Master’s in Artificial Intelligence, University at Buffalo  

**Nandhakumar Vadivel**  
Master’s in Artificial Intelligence, University at Buffalo

